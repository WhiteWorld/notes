# Reading Notes

交叉熵代价函数

### CS229 notes

Notes1:

线性回归：

* 最小二乘法、梯度下降（批量梯度下降、随机梯度下降\)、

* 一般方程（使用矩阵方式，类似投影矩阵）

* 概率解释（最大似然函数，对数似然函数，推出最小二乘法）

* 本地加权线性回归 LWR


分类和 logistic 回归

* LR

* PLA 算法

* 牛顿法求极值


通用线性模型

* 指数簇

* 构造GLMs

* 

Notes2:

判别学习算法

生成学习算法 GLM

* GDA  高斯判决模型
* 朴素贝叶斯
  * 假设：特征对标签是条件无关的
  * 分类器，连续数据离散化再应用
  * 拉普拉斯滤波，防止出现没有在训练数据中出现的特征，分子和分母加入参数


### Deep Learning Book

Deep feedforward networks \(MLPs\)

BP 算法：

计算每个参数w对误差E的影响程度，即求对E进行每个w的偏导，求出偏导然后乘以学习因子，进行参数更新。

具体步骤，

1. 先从前往后依次计算各层各个神经元的激活值

2. 计算输出层各个神经元的残差（残差就是误差E对神经元输入的偏导，这个残差可以通过微分的链式法则进而求得误差E对参数w的偏导）

3. 由后向前，依次计算出各个神经元的残差
4. 计算各层各个参数w和b的梯度（针对一个样本）
5. 计算各层各个参数w和b的平均梯度（针对m个样本的步骤1~4获取的值求平均，加上一个正则化项）
6. 通过学习因子乘以梯度，更新各个参数w和b
7. 直到误差足够小，停止

### 机器学习实战

监督学习: 机器从数据中获取目标变量。目标变量分为两种：离散变量（分类），连续变量（回归）
无监督学习：聚类，输入数据无标签

机器学习程序开发步骤

* 收集数据
* 准备输入数据
* 分析输入数据
* 人工参与
* 训练算法
* 测试算法
* 使用算法

k-近邻算法 kNN

* 优点：简单高效
* 缺点：保存全部数据集，计算需要存储空间大，耗时

为了提高K近邻搜索的效率，可以考虑kd树的方法。

决策树

* 决策树的构造
  * 信息增益
  * 划分数据集
  * 递归构造树
  * 决策树的使用
    * 序列化到磁盘



朴素贝叶斯

条件概率

Logistic 回归

优点：代价不高，易于实现
缺点：容易欠拟合，分类精度可能不高

推到过程：[基于Logistic回归的二元分类应用（含公式推导）](http://www.jianshu.com/p/9ffab4c4f76d)

Softmax 算法：多分类算法，Logistic 回归的一般化

SVM 支持向量机

[支持向量机系列](http://blog.pluskid.org/?page_id=683)
[支持向量机SVM](http://www.cnblogs.com/jerrylead/archive/2011/03/13/1982639.html)

线性分类器-&gt;转化为对偶问题并优化求解-&gt;
最大间隔

Adaboost

[ 数据挖掘算法学习（八）Adaboost算法](http://blog.csdn.net/iemyxie/article/details/40423907)

